{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "eb7ef720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "import librosa\n",
    "import mido\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8b699464",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, time_count: int = 26, note_count: int = 110, vel_count: int = 2):\n",
    "        self.val_to_velo_id: dict = {i: i + 1 for i in range(vel_count)}\n",
    "        self.val_to_note_id: dict = {i: i + 1 +\n",
    "                                     vel_count for i in range(note_count)}\n",
    "        self.val_to_time_id: dict = {\n",
    "            i: i + 1 + vel_count + note_count for i in range(time_count)}\n",
    "\n",
    "        self.velo_id_to_val: dict = {\n",
    "            v: k for k, v in self.val_to_velo_id.items()}\n",
    "        self.note_id_to_val: dict = {\n",
    "            v: k for k, v in self.val_to_note_id.items()}\n",
    "        self.time_id_to_val: dict = {\n",
    "            v: k for k, v in self.val_to_time_id.items()}\n",
    "\n",
    "        self.id_to_token: dict = {\n",
    "            **{self.val_to_velo_id[i]: f'velo_{i}' for i in self.val_to_velo_id},\n",
    "            **{self.val_to_note_id[i]: f'note_{i}' for i in self.val_to_note_id},\n",
    "            **{self.val_to_time_id[i]: f'time_{i}' for i in self.val_to_time_id},\n",
    "            0: '<pad>',\n",
    "            vel_count + note_count + time_count + 1: '<bos>',\n",
    "            vel_count + note_count + time_count + 2: '<eos>'\n",
    "        }\n",
    "\n",
    "        self.token_to_id: dict = {v: k for k, v in self.id_to_token.items()}\n",
    "\n",
    "    def tuple_to_ids(self, tuple: tuple):\n",
    "        return [self.val_to_time_id[tuple[0]], self.val_to_note_id[tuple[1]], self.val_to_velo_id[tuple[2]]]\n",
    "\n",
    "    def tuple_list_to_ids(self, tuple_list: list[tuple]):\n",
    "        l = []\n",
    "        for t in tuple_list:\n",
    "            l.extend(self.tuple_to_ids(t))\n",
    "        return l\n",
    "\n",
    "    def id_list_to_tuple_list(self, id_list: list[int]):\n",
    "        l = []\n",
    "        for i in range(0, len(id_list), 3):\n",
    "            if i + 3 > len(id_list):\n",
    "                break\n",
    "            t = []\n",
    "            for j, d in enumerate([self.time_id_to_val, self.note_id_to_val, self.velo_id_to_val]):\n",
    "                if min(d) <= id_list[i+j] <= max(d):\n",
    "                    t.append(d[id_list[i+j]])\n",
    "                else:\n",
    "                    t.append(-1)\n",
    "            l.append(tuple(t))\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "22098447",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressBar:\n",
    "    def __init__(self, total, length=40):\n",
    "        self.total = total\n",
    "        self.length = length\n",
    "        self.current = 0\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def update(self, step=1):\n",
    "        self.current += step\n",
    "        progress = self.current / self.total\n",
    "        filled_length = int(self.length * progress)\n",
    "        bar = '=' * filled_length + '-' * (self.length - filled_length)\n",
    "        if self.current == 1 or self.current == self.total or filled_length > int(self.length * ((self.current - step) / self.total)):\n",
    "            print(f'\\r|{bar}| {self.current}/{self.total} ({progress:.2%})  {time.time() - self.start_time:.1f}s', end='')\n",
    "\n",
    "    # def finish(self):\n",
    "    #     self.update(self.total - self.current)\n",
    "    #     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "04752e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator():\n",
    "    def __init__(\n",
    "        self,\n",
    "        wav_mid_df: pd.DataFrame,\n",
    "        output_path: str,\n",
    "        chunk_size: int = 128,\n",
    "        step_size: int = 64,\n",
    "        sample_rate: int = 12_800,\n",
    "        n_fft: int = 2048,\n",
    "        hop_length: int = 256,\n",
    "        n_mels: int = 512,\n",
    "        override: bool = False,\n",
    "        # transform: transforms.Compose = None\n",
    "    ):\n",
    "        self.wav_mid_df = wav_mid_df\n",
    "        self.output_path = output_path\n",
    "        self.chunk_size = chunk_size\n",
    "        self.step_size = step_size\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.n_mels = n_mels\n",
    "        # self.transform = transform\n",
    "        self.time_per_frame = hop_length / sample_rate\n",
    "\n",
    "        if override and os.path.exists(self.output_path):\n",
    "            os.remove(self.output_path)\n",
    "\n",
    "    def _get_spectogram(self, wave_path: str) -> np.ndarray:\n",
    "        samples, sr = librosa.load(wave_path, sr=self.sample_rate)\n",
    "\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(\n",
    "            y=samples, sr=sr, n_fft=self.n_fft, hop_length=self.hop_length, n_mels=self.n_mels)\n",
    "        mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "        return mel_spectrogram_db\n",
    "\n",
    "    def _get_message_df(self, midi_path) -> pd.DataFrame:\n",
    "        mid = mido.MidiFile(midi_path)\n",
    "        note_msg = map(lambda x: (x, x.time), mid.tracks[-1])\n",
    "        note_df = pd.DataFrame(note_msg, columns=['other', 'time'])\n",
    "\n",
    "        note_df['time'] = note_df['time'].cumsum()\n",
    "        note_df = note_df[note_df['other'].apply(\n",
    "            lambda x: x.type == 'note_on')]\n",
    "        note_df['note'] = note_df['other'].apply(lambda x: x.note)\n",
    "        note_df['velocity'] = note_df['other'].apply(lambda x: x.velocity)\n",
    "        note_df = note_df.drop(columns=['other']).reset_index(drop=True)\n",
    "        return note_df\n",
    "    \n",
    "    def _get_spec_chunk(self, spec: np.ndarray, i: int) -> np.ndarray:\n",
    "        spec_chunk = spec[:, i * self.step_size : i * self.step_size + self.chunk_size]\n",
    "        return spec_chunk\n",
    "\n",
    "    def _get_midi_chunk(self, midi: pd.DataFrame, i: int) -> np.ndarray:\n",
    "        start_time = i * self.step_size * self.time_per_frame * 1000\n",
    "        end_time = (i * self.step_size + self.chunk_size) * self.time_per_frame * 1000\n",
    "\n",
    "        midi_chunk = midi[(midi['time'] >= start_time) & (midi['time'] < end_time)].copy()\n",
    "        midi_chunk['time'] = midi_chunk['time'] - int(start_time)\n",
    "        return midi_chunk.to_numpy()\n",
    "    \n",
    "    def _get_chunk_meta(self, midi_chunk: np.ndarray):\n",
    "        if midi_chunk.shape[0] == 0:\n",
    "            return np.array([0, 0, 0])\n",
    "        meta = [midi_chunk[0].min(), midi_chunk[0].max(), len(midi_chunk)]\n",
    "        return meta\n",
    "    \n",
    "    def _get_chunks(self, wave_path: str, midi_path: str) -> pd.DataFrame:\n",
    "        spec = self._get_spectogram(wave_path)\n",
    "        midi = self._get_message_df(midi_path)\n",
    "\n",
    "        length = int((spec.shape[1] - self.chunk_size) / self.step_size)\n",
    "        chunks = []\n",
    "        for i in range(length):\n",
    "            spec_chunk = self._get_spec_chunk(spec, i)\n",
    "            midi_chunk = self._get_midi_chunk(midi, i)\n",
    "            \n",
    "            chunks.append((spec_chunk, midi_chunk))\n",
    "        return pd.DataFrame(chunks, columns=['spec', 'midi'])\n",
    "\n",
    "    def _save_chunk(self, spec_chunk: np.ndarray, midi_chunk: np.ndarray, piece_name: str):\n",
    "        with h5py.File(self.output_path, 'a') as h5:\n",
    "            if piece_name not in h5:\n",
    "                piece_group = h5.create_group(piece_name)\n",
    "            else:\n",
    "                piece_group = h5[piece_name]\n",
    "\n",
    "            chunk_idx = len(piece_group)\n",
    "            chunk_group = piece_group.create_group(f'chunk_{chunk_idx}')\n",
    "\n",
    "            chunk_group.create_dataset('image', data=spec_chunk, compression='gzip')\n",
    "            chunk_group.create_dataset('midi', data=midi_chunk, compression='gzip')\n",
    "            chunk_group.create_dataset('meta', data=self._get_chunk_meta(midi_chunk), compression='gzip')\n",
    "\n",
    "    def generate(self):\n",
    "        output_dir = os.path.dirname(self.output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        progress_bar = ProgressBar(len(self.wav_mid_df))\n",
    "        for i, (idx, row) in enumerate(self.wav_mid_df.iterrows()):\n",
    "            wave_path = row['wav']\n",
    "            midi_path = row['midi']\n",
    "            piece_name = os.path.basename(wave_path).split('.')[0]\n",
    "\n",
    "            if os.path.exists(wave_path) and os.path.exists(midi_path):\n",
    "                chunks = self._get_chunks(wave_path, midi_path)\n",
    "                for _, (spec_chunk, midi_chunk) in chunks.iterrows():\n",
    "                    self._save_chunk(spec_chunk, midi_chunk, piece_name)\n",
    "            else:\n",
    "                print(f\"File not found: {wave_path} or {midi_path}\")\n",
    "\n",
    "            progress_bar.update()\n",
    "\n",
    "\n",
    "    # def _getSimpleSpectogram(self, wave_file: str, notes: list[int]):\n",
    "    #     spec = self.getSpectogram(wave_file)\n",
    "    #     mel_frequencies = librosa.mel_frequencies(n_mels=spec.shape[0], fmin=0, fmax=self.sample_rate / 2)\n",
    "\n",
    "    #     note_freqs = pd.read_csv('note_freqs.csv').values\n",
    "    #     indexes = np.array([mel_frequencies[np.abs(mel_frequencies - val).argmin()] for val in note_freqs])\n",
    "    #     indexes = np.array([np.where(mel_frequencies == val)[0][0] for val in indexes])\n",
    "    #     indexes = indexes[notes]\n",
    "\n",
    "    #     spec = spec[indexes]\n",
    "    #     return spec, mel_frequencies[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "68d29a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|========================================| 10/10 (100.00%)  15.6s"
     ]
    }
   ],
   "source": [
    "csv_path = 'wav_midi.csv'\n",
    "h5_path = f'dataset_transformed.h5'\n",
    "folder_path = 'waves'\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "df['wav'] = df['wav'].apply(lambda x: os.path.join(folder_path, x))\n",
    "df['midi'] = df['midi'].apply(lambda x: os.path.join(folder_path, x))\n",
    "df = df.sample(10)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "])\n",
    "\n",
    "dataset_gen = DatasetGenerator(df, h5_path, override=True)\n",
    "dataset_gen.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "06d57a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|========================================| 10/10 (100.00%)  4.3s"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "with h5py.File(h5_path, 'r') as h5:\n",
    "    progress_bar = ProgressBar(len(h5.keys()))\n",
    "    for piece_name in h5.keys():\n",
    "        piece_group = h5[piece_name]\n",
    "        for chunk_name in piece_group.keys():\n",
    "            chunk_group = piece_group[chunk_name]\n",
    "            spec = chunk_group['image'][:]\n",
    "            midi = chunk_group['midi'][:]\n",
    "            meta = chunk_group['meta'][:]\n",
    "            spec = Image.fromarray(spec)\n",
    "            spec = transform(spec)\n",
    "            midi[:, 0] = midi[:, 0] // 100\n",
    "            midi[:, 2] = (midi[:, 2] > 0).astype(np.uint8)\n",
    "            midi = midi.tolist()\n",
    "            midi = tokenizer.tuple_list_to_ids(midi)\n",
    "            midi.insert(0, tokenizer.token_to_id['<bos>'])\n",
    "            midi.append(tokenizer.token_to_id['<eos>'])\n",
    "            midi.extend([tokenizer.token_to_id['<pad>']] * (1100 - len(midi)))\n",
    "\n",
    "        progress_bar.update()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "midi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
